# This file has been generated by Bovinus from a grammar file.
# (See http://bovinus.bollmeier.de for details)
# All changes outside of editable sections will be overwritten.

import bovinus.token as token
import bovinus.grammar as grammar
import bovinus.parser as parser
from bovinus.parser import AstNode

# edit-section init {
# } edit-section-end

class GobjcreatorParser(parser.Parser):

	def __init__(self):
		
		parser.Parser.__init__(self, _GobjcreatorGrammar())
		
		self.enableLineComments('//')
		self.enableBlockComments('/*', '*/')
		

# ========== Private section ==========

all_token_types = []

LITERAL = token.Literal.get()
all_token_types.append(LITERAL)

ID = token.Word('[a-zA-Z_][a-zA-Z0-9_]*')
all_token_types.append(ID)

G_TYPE_NAME = token.Word('[A-Z][A-Z0-9_]*')
all_token_types.append(G_TYPE_NAME)

FILE_PATH = token.Word('[^>]+')
all_token_types.append(FILE_PATH)

UINT = token.Word('[1-9][0-9]*|0')
all_token_types.append(UINT)

PARENT_MODULE = token.Keyword('..', caseSensitive=True)
all_token_types.append(PARENT_MODULE)

LRARROW = token.Separator('<->', whitespaceAllowed=True, escape=True)
all_token_types.append(LRARROW)

LARROW = token.Separator('<-', whitespaceAllowed=True, escape=True)
all_token_types.append(LARROW)

RARROW = token.Separator('->', whitespaceAllowed=True, escape=True)
all_token_types.append(RARROW)

ASSIGN = token.Separator('=', whitespaceAllowed=True, escape=True)
all_token_types.append(ASSIGN)

COLON = token.Separator(':', whitespaceAllowed=True, escape=True)
all_token_types.append(COLON)

SEMICOLON = token.Separator(';', whitespaceAllowed=True, escape=True)
all_token_types.append(SEMICOLON)

COMMA = token.Separator(',', whitespaceAllowed=True, escape=True)
all_token_types.append(COMMA)

DASH = token.Separator('-', whitespaceAllowed=True, escape=True)
all_token_types.append(DASH)

SLASH = token.Separator('/', whitespaceAllowed=True, escape=True)
all_token_types.append(SLASH)

LSBRACKET = token.Separator('[', whitespaceAllowed=True, escape=True)
all_token_types.append(LSBRACKET)

RSBRACKET = token.Separator(']', whitespaceAllowed=True, escape=True)
all_token_types.append(RSBRACKET)

LBRACE = token.Separator('{', whitespaceAllowed=True, escape=True)
all_token_types.append(LBRACE)

RBRACE = token.Separator('}', whitespaceAllowed=True, escape=True)
all_token_types.append(RBRACE)

LPAR = token.Separator('(', whitespaceAllowed=True, escape=True)
all_token_types.append(LPAR)

RPAR = token.Separator(')', whitespaceAllowed=True, escape=True)
all_token_types.append(RPAR)

LABRACKET = token.Separator('<', whitespaceAllowed=True, escape=True)
all_token_types.append(LABRACKET)

RABRACKET = token.Separator('>', whitespaceAllowed=True, escape=True)
all_token_types.append(RABRACKET)

KEY_1 = token.Keyword('include', caseSensitive=True)
all_token_types.append(KEY_1)

KEY_2 = token.Keyword('typedecl', caseSensitive=True)
all_token_types.append(KEY_2)

KEY_3 = token.Keyword('module', caseSensitive=True)
all_token_types.append(KEY_3)

KEY_4 = token.Keyword('gobject', caseSensitive=True)
all_token_types.append(KEY_4)

KEY_5 = token.Keyword('ginterface', caseSensitive=True)
all_token_types.append(KEY_5)

KEY_6 = token.Keyword('gerror', caseSensitive=True)
all_token_types.append(KEY_6)

KEY_7 = token.Keyword('genum', caseSensitive=True)
all_token_types.append(KEY_7)

KEY_8 = token.Keyword('gflags', caseSensitive=True)
all_token_types.append(KEY_8)

KEY_9 = token.Keyword('inherits', caseSensitive=True)
all_token_types.append(KEY_9)

KEY_10 = token.Keyword('from', caseSensitive=True)
all_token_types.append(KEY_10)

KEY_11 = token.Keyword('implements', caseSensitive=True)
all_token_types.append(KEY_11)

KEY_12 = token.Keyword('properties', caseSensitive=True)
all_token_types.append(KEY_12)

KEY_13 = token.Keyword('type', caseSensitive=True)
all_token_types.append(KEY_13)

KEY_14 = token.Keyword('boolean', caseSensitive=True)
all_token_types.append(KEY_14)

KEY_15 = token.Keyword('byte', caseSensitive=True)
all_token_types.append(KEY_15)

KEY_16 = token.Keyword('integer', caseSensitive=True)
all_token_types.append(KEY_16)

KEY_17 = token.Keyword('float', caseSensitive=True)
all_token_types.append(KEY_17)

KEY_18 = token.Keyword('double', caseSensitive=True)
all_token_types.append(KEY_18)

KEY_19 = token.Keyword('string', caseSensitive=True)
all_token_types.append(KEY_19)

KEY_20 = token.Keyword('pointer', caseSensitive=True)
all_token_types.append(KEY_20)

KEY_21 = token.Keyword('object', caseSensitive=True)
all_token_types.append(KEY_21)

KEY_22 = token.Keyword('enumeration', caseSensitive=True)
all_token_types.append(KEY_22)

KEY_23 = token.Keyword('access', caseSensitive=True)
all_token_types.append(KEY_23)

KEY_24 = token.Keyword('read', caseSensitive=True)
all_token_types.append(KEY_24)

KEY_25 = token.Keyword('only', caseSensitive=True)
all_token_types.append(KEY_25)

KEY_26 = token.Keyword('write', caseSensitive=True)
all_token_types.append(KEY_26)

KEY_27 = token.Keyword('initial', caseSensitive=True)
all_token_types.append(KEY_27)

KEY_28 = token.Keyword('description', caseSensitive=True)
all_token_types.append(KEY_28)

KEY_29 = token.Keyword('gtype', caseSensitive=True)
all_token_types.append(KEY_29)

KEY_30 = token.Keyword('gtypeof', caseSensitive=True)
all_token_types.append(KEY_30)

KEY_31 = token.Keyword('signals', caseSensitive=True)
all_token_types.append(KEY_31)

KEY_32 = token.Keyword('methods', caseSensitive=True)
all_token_types.append(KEY_32)

KEY_33 = token.Keyword('attributes', caseSensitive=True)
all_token_types.append(KEY_33)

KEY_34 = token.Keyword('public', caseSensitive=True)
all_token_types.append(KEY_34)

KEY_35 = token.Keyword('protected', caseSensitive=True)
all_token_types.append(KEY_35)

KEY_36 = token.Keyword('private', caseSensitive=True)
all_token_types.append(KEY_36)

KEY_37 = token.Keyword('static', caseSensitive=True)
all_token_types.append(KEY_37)

KEY_38 = token.Keyword('abstract', caseSensitive=True)
all_token_types.append(KEY_38)

KEY_39 = token.Keyword('overridden', caseSensitive=True)
all_token_types.append(KEY_39)

KEY_40 = token.Keyword('final', caseSensitive=True)
all_token_types.append(KEY_40)

KEY_41 = token.Keyword('ref', caseSensitive=True)
all_token_types.append(KEY_41)

KEY_42 = token.Keyword('list', caseSensitive=True)
all_token_types.append(KEY_42)

KEY_43 = token.Keyword('unsigned', caseSensitive=True)
all_token_types.append(KEY_43)

KEY_44 = token.Keyword('any', caseSensitive=True)
all_token_types.append(KEY_44)

class _PropTypeRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'prop_type', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		return AstNode('type', astNode['#value'].getText())
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_13)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ASSIGN)
		
	def _sub_1_3(self):
		
		branches = []
		branches.append(self._sub_1_3_1())
		branches.append(self._sub_1_3_2())
		branches.append(self._sub_1_3_3())
		branches.append(self._sub_1_3_4())
		branches.append(self._sub_1_3_5())
		branches.append(self._sub_1_3_6())
		branches.append(self._sub_1_3_7())
		branches.append(self._sub_1_3_8())
		branches.append(self._sub_1_3_9())
		
		return grammar.Fork(branches)
		
	def _sub_1_3_1(self):
		
		return self._sub_1_3_1_1()
		
	def _sub_1_3_1_1(self):
		
		return grammar.tokenNode(KEY_14, 'value')
		
	def _sub_1_3_2(self):
		
		return self._sub_1_3_2_1()
		
	def _sub_1_3_2_1(self):
		
		return grammar.tokenNode(KEY_15, 'value')
		
	def _sub_1_3_3(self):
		
		return self._sub_1_3_3_1()
		
	def _sub_1_3_3_1(self):
		
		return grammar.tokenNode(KEY_16, 'value')
		
	def _sub_1_3_4(self):
		
		return self._sub_1_3_4_1()
		
	def _sub_1_3_4_1(self):
		
		return grammar.tokenNode(KEY_17, 'value')
		
	def _sub_1_3_5(self):
		
		return self._sub_1_3_5_1()
		
	def _sub_1_3_5_1(self):
		
		return grammar.tokenNode(KEY_18, 'value')
		
	def _sub_1_3_6(self):
		
		return self._sub_1_3_6_1()
		
	def _sub_1_3_6_1(self):
		
		return grammar.tokenNode(KEY_19, 'value')
		
	def _sub_1_3_7(self):
		
		return self._sub_1_3_7_1()
		
	def _sub_1_3_7_1(self):
		
		return grammar.tokenNode(KEY_20, 'value')
		
	def _sub_1_3_8(self):
		
		return self._sub_1_3_8_1()
		
	def _sub_1_3_8_1(self):
		
		return grammar.tokenNode(KEY_21, 'value')
		
	def _sub_1_3_9(self):
		
		return self._sub_1_3_9_1()
		
	def _sub_1_3_9_1(self):
		
		return grammar.tokenNode(KEY_22, 'value')
		
class _ModuleNameRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'module_name', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		
		res = AstNode(self.getName())
		
		for child in astNode.getChildrenById('name'):
			res.addChild(AstNode('part', child.getText()))
		
		return res
		
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_2(self):
		
		return grammar.zeroToMany(self._sub_1_2_1())
		
	def _sub_1_2_1(self):
		
		elements = []
		elements.append(self._sub_1_2_1_1())
		elements.append(self._sub_1_2_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_2_1_1(self):
		
		return grammar.tokenNode(SLASH)
		
	def _sub_1_2_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
class _AttrPropertyRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'attr_property', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		name = astNode.getChildren()[0].getText()
		
		return AstNode(name)
		
	def _sub_1(self):
		
		return self._sub_1_1()
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_37)
		
class _GflagsRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'gflags', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		name = astNode['#name']
		res.addChild(AstNode('name', name.getText()))
		
		for code in astNode.getChildrenById('code'):
			res.addChild(AstNode('code', code.getText()))
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		elements.append(self._sub_1_6())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_8)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(ID, 'code')
		
	def _sub_1_5(self):
		
		return grammar.zeroToMany(self._sub_1_5_1())
		
	def _sub_1_5_1(self):
		
		elements = []
		elements.append(self._sub_1_5_1_1())
		elements.append(self._sub_1_5_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_5_1_1(self):
		
		return grammar.tokenNode(COMMA)
		
	def _sub_1_5_1_2(self):
		
		return grammar.tokenNode(ID, 'code')
		
	def _sub_1_6(self):
		
		return grammar.tokenNode(RBRACE)
		
class _InclpathRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'inclpath', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		start.connect(self._sub_2()).connect(end)
		
	def transform(self, astNode):
		
		
		res = AstNode(self.getName())
		
		children = astNode.getChildren()
		if len(children) == 1:
			text = children[0].getText()[1:-1]
			res.addChild(AstNode('name', text))
		else:
			text = children[1].getText()
			res.addChild(AstNode('name', text))
			res.addChild(AstNode('standard'))
		
		return res
		
	def _sub_1(self):
		
		return self._sub_1_1()
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(LITERAL)
		
	def _sub_2(self):
		
		elements = []
		elements.append(self._sub_2_1())
		elements.append(self._sub_2_2())
		elements.append(self._sub_2_3())
		
		return grammar.Sequence(elements)
		
	def _sub_2_1(self):
		
		return grammar.tokenNode(LABRACKET)
		
	def _sub_2_2(self):
		
		return grammar.tokenNode(FILE_PATH)
		
	def _sub_2_3(self):
		
		return grammar.tokenNode(RABRACKET)
		
class _PropAccessRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'prop_access', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		text = ""
		for child in astNode.getChildrenById('value'):
			text += child.getText()
		
		return AstNode('access', text)
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_23)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ASSIGN)
		
	def _sub_1_3(self):
		
		branches = []
		branches.append(self._sub_1_3_1())
		branches.append(self._sub_1_3_2())
		branches.append(self._sub_1_3_3())
		
		return grammar.Fork(branches)
		
	def _sub_1_3_1(self):
		
		elements = []
		elements.append(self._sub_1_3_1_1())
		elements.append(self._sub_1_3_1_2())
		elements.append(self._sub_1_3_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_1_1(self):
		
		return grammar.tokenNode(KEY_24, 'value')
		
	def _sub_1_3_1_2(self):
		
		return grammar.tokenNode(DASH, 'value')
		
	def _sub_1_3_1_3(self):
		
		return grammar.tokenNode(KEY_25, 'value')
		
	def _sub_1_3_2(self):
		
		elements = []
		elements.append(self._sub_1_3_2_1())
		elements.append(self._sub_1_3_2_2())
		elements.append(self._sub_1_3_2_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_2_1(self):
		
		return grammar.tokenNode(KEY_24, 'value')
		
	def _sub_1_3_2_2(self):
		
		return grammar.tokenNode(DASH, 'value')
		
	def _sub_1_3_2_3(self):
		
		return grammar.tokenNode(KEY_26, 'value')
		
	def _sub_1_3_3(self):
		
		elements = []
		elements.append(self._sub_1_3_3_1())
		elements.append(self._sub_1_3_3_2())
		elements.append(self._sub_1_3_3_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_3_1(self):
		
		return grammar.tokenNode(KEY_27, 'value')
		
	def _sub_1_3_3_2(self):
		
		return grammar.tokenNode(DASH, 'value')
		
	def _sub_1_3_3_3(self):
		
		return grammar.tokenNode(KEY_26, 'value')
		
class _BuiltinTypeRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'builtin_type', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		start.connect(self._sub_2()).connect(end)
		start.connect(self._sub_3()).connect(end)
		start.connect(self._sub_4()).connect(end)
		start.connect(self._sub_5()).connect(end)
		start.connect(self._sub_6()).connect(end)
		
	def transform(self, astNode):
		
		typename=""
		for child in astNode.getChildren():
			if typename:
				typename += " "
			typename += child.getText()
		
		return AstNode(self.getName(), typename)
		
	def _sub_1(self):
		
		return self._sub_1_1()
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_19)
		
	def _sub_2(self):
		
		return self._sub_2_1()
		
	def _sub_2_1(self):
		
		return grammar.tokenNode(KEY_14)
		
	def _sub_3(self):
		
		elements = []
		elements.append(self._sub_3_1())
		elements.append(self._sub_3_2())
		
		return grammar.Sequence(elements)
		
	def _sub_3_1(self):
		
		return grammar.zeroToOne(grammar.tokenNode(KEY_43))
		
	def _sub_3_2(self):
		
		return grammar.tokenNode(KEY_16)
		
	def _sub_4(self):
		
		return self._sub_4_1()
		
	def _sub_4_1(self):
		
		return grammar.tokenNode(KEY_17)
		
	def _sub_5(self):
		
		return self._sub_5_1()
		
	def _sub_5_1(self):
		
		return grammar.tokenNode(KEY_18)
		
	def _sub_6(self):
		
		return self._sub_6_1()
		
	def _sub_6_1(self):
		
		return grammar.tokenNode(KEY_44)
		
class _VisibilityRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'visibility', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		start.connect(self._sub_2()).connect(end)
		start.connect(self._sub_3()).connect(end)
		
	def transform(self, astNode):
		
		return AstNode(self.getName(), astNode.getChildren()[0].getText())
		
	def _sub_1(self):
		
		return self._sub_1_1()
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_34)
		
	def _sub_2(self):
		
		return self._sub_2_1()
		
	def _sub_2_1(self):
		
		return grammar.tokenNode(KEY_35)
		
	def _sub_3(self):
		
		return self._sub_3_1()
		
	def _sub_3_1(self):
		
		return grammar.tokenNode(KEY_36)
		
class _NameWDashesRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'name_w_dashes', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		text = ""
		for child in astNode.getChildren():
			text += child.getText()
		return AstNode('name', text)
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(ID)
		
	def _sub_1_2(self):
		
		return grammar.zeroToMany(self._sub_1_2_1())
		
	def _sub_1_2_1(self):
		
		elements = []
		elements.append(self._sub_1_2_1_1())
		elements.append(self._sub_1_2_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_2_1_1(self):
		
		return grammar.tokenNode(DASH)
		
	def _sub_1_2_1_2(self):
		
		return grammar.tokenNode(ID)
		
class _TypeDeclarationRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'type_declaration', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		
		return AstNode(self.getName(), astNode['#name'].getText())
		
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_2)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(SEMICOLON)
		
class _PropDescriptionRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'prop_description', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		value = astNode.getChildById('value')
		
		return AstNode('description', value.getText())
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_28)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ASSIGN)
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LITERAL, 'value')
		
class _EnumCodeRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'enum_code', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('enum_code')
		
		name = AstNode('name', astNode['#code'].getText())
		res.addChild(name)
		
		val = astNode['#value']
		if val:
			res.addChild(AstNode('value', val.getText()))
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(ID, 'code')
		
	def _sub_1_2(self):
		
		return grammar.zeroToOne(self._sub_1_2_1())
		
	def _sub_1_2_1(self):
		
		elements = []
		elements.append(self._sub_1_2_1_1())
		elements.append(self._sub_1_2_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_2_1_1(self):
		
		return grammar.tokenNode(ASSIGN)
		
	def _sub_1_2_1_2(self):
		
		return grammar.tokenNode(UINT, 'value')
		
class _GerrorRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'gerror', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('gerror')
		
		name = astNode['#name']
		res.addChild(AstNode('name', name.getText()))
		
		for code in astNode.getChildrenById('code'):
			res.addChild(AstNode('code', code.getText()))
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		elements.append(self._sub_1_6())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_6)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(ID, 'code')
		
	def _sub_1_5(self):
		
		return grammar.zeroToMany(self._sub_1_5_1())
		
	def _sub_1_5_1(self):
		
		elements = []
		elements.append(self._sub_1_5_1_1())
		elements.append(self._sub_1_5_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_5_1_1(self):
		
		return grammar.tokenNode(COMMA)
		
	def _sub_1_5_1_2(self):
		
		return grammar.tokenNode(ID, 'code')
		
	def _sub_1_6(self):
		
		return grammar.tokenNode(RBRACE)
		
class _FullTypeNameRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'full_type_name', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		if astNode['#root']:
			res.addChild(AstNode('absolute_type'))
		
		for m in astNode.getChildrenById('module'):
			res.addChild(AstNode('module', m.getText()))
		
		res.addChild(AstNode('name', astNode['#name'].getText()))
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.zeroToOne(grammar.tokenNode(SLASH, 'root'))
		
	def _sub_1_2(self):
		
		return grammar.zeroToOne(self._sub_1_2_1())
		
	def _sub_1_2_1(self):
		
		elements = []
		elements.append(self._sub_1_2_1_1())
		elements.append(self._sub_1_2_1_2())
		elements.append(self._sub_1_2_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_2_1_1(self):
		
		branches = []
		branches.append(self._sub_1_2_1_1_1())
		branches.append(self._sub_1_2_1_1_2())
		
		return grammar.Fork(branches)
		
	def _sub_1_2_1_1_1(self):
		
		return self._sub_1_2_1_1_1_1()
		
	def _sub_1_2_1_1_1_1(self):
		
		return grammar.tokenNode(ID, 'module')
		
	def _sub_1_2_1_1_2(self):
		
		return self._sub_1_2_1_1_2_1()
		
	def _sub_1_2_1_1_2_1(self):
		
		return grammar.tokenNode(PARENT_MODULE, 'module')
		
	def _sub_1_2_1_2(self):
		
		return grammar.zeroToMany(self._sub_1_2_1_2_1())
		
	def _sub_1_2_1_2_1(self):
		
		elements = []
		elements.append(self._sub_1_2_1_2_1_1())
		elements.append(self._sub_1_2_1_2_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_2_1_2_1_1(self):
		
		return grammar.tokenNode(SLASH)
		
	def _sub_1_2_1_2_1_2(self):
		
		branches = []
		branches.append(self._sub_1_2_1_2_1_2_1())
		branches.append(self._sub_1_2_1_2_1_2_2())
		
		return grammar.Fork(branches)
		
	def _sub_1_2_1_2_1_2_1(self):
		
		return self._sub_1_2_1_2_1_2_1_1()
		
	def _sub_1_2_1_2_1_2_1_1(self):
		
		return grammar.tokenNode(ID, 'module')
		
	def _sub_1_2_1_2_1_2_2(self):
		
		return self._sub_1_2_1_2_1_2_2_1()
		
	def _sub_1_2_1_2_1_2_2_1(self):
		
		return grammar.tokenNode(PARENT_MODULE, 'module')
		
	def _sub_1_2_1_3(self):
		
		return grammar.tokenNode(SLASH)
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(ID, 'name')
		
class _MethodPropertyRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'method_property', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		start.connect(self._sub_2()).connect(end)
		start.connect(self._sub_3()).connect(end)
		start.connect(self._sub_4()).connect(end)
		
	def transform(self, astNode):
		
		name = astNode.getChildren()[0].getText()
		
		return AstNode(name)
		
	def _sub_1(self):
		
		return self._sub_1_1()
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_37)
		
	def _sub_2(self):
		
		return self._sub_2_1()
		
	def _sub_2_1(self):
		
		return grammar.tokenNode(KEY_38)
		
	def _sub_3(self):
		
		return self._sub_3_1()
		
	def _sub_3_1(self):
		
		return grammar.tokenNode(KEY_39)
		
	def _sub_4(self):
		
		return self._sub_4_1()
		
	def _sub_4_1(self):
		
		return grammar.tokenNode(KEY_40)
		
class _PropGtypeRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'prop_gtype', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		
		res = AstNode('gtype')
		
		idNode = astNode['#id']
		if idNode:
			res.addChild(AstNode('id', idNode.getText()))
		else:
			child = AstNode('type_of')
			typeNameNode = astNode['#typeName']
			typeNameNode.setId('')
			child.addChild(typeNameNode)
			res.addChild(child)
		
		return res
		
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_29)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ASSIGN)
		
	def _sub_1_3(self):
		
		branches = []
		branches.append(self._sub_1_3_1())
		branches.append(self._sub_1_3_2())
		
		return grammar.Fork(branches)
		
	def _sub_1_3_1(self):
		
		return self._sub_1_3_1_1()
		
	def _sub_1_3_1_1(self):
		
		return grammar.tokenNode(G_TYPE_NAME, 'id')
		
	def _sub_1_3_2(self):
		
		elements = []
		elements.append(self._sub_1_3_2_1())
		elements.append(self._sub_1_3_2_2())
		elements.append(self._sub_1_3_2_3())
		elements.append(self._sub_1_3_2_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_2_1(self):
		
		return grammar.tokenNode(KEY_30)
		
	def _sub_1_3_2_2(self):
		
		return grammar.tokenNode(LPAR)
		
	def _sub_1_3_2_3(self):
		
		return _FullTypeNameRule('typeName')
		
	def _sub_1_3_2_4(self):
		
		return grammar.tokenNode(RPAR)
		
class _TypeNameRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'type_name', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		start.connect(self._sub_2()).connect(end)
		
	def transform(self, astNode):
		
		return astNode.getChildren()[0]
		
	def _sub_1(self):
		
		return self._sub_1_1()
		
	def _sub_1_1(self):
		
		return _BuiltinTypeRule()
		
	def _sub_2(self):
		
		return self._sub_2_1()
		
	def _sub_2_1(self):
		
		return _FullTypeNameRule()
		
class _IncludeStmtRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'include_stmt', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('include')
		
		for path in astNode.getChildrenById('name'):
			path.setId('')
			res.addChild(path)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_1)
		
	def _sub_1_2(self):
		
		return _InclpathRule('name')
		
	def _sub_1_3(self):
		
		return grammar.zeroToMany(self._sub_1_3_1())
		
	def _sub_1_3_1(self):
		
		elements = []
		elements.append(self._sub_1_3_1_1())
		elements.append(self._sub_1_3_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_1_1(self):
		
		return grammar.tokenNode(COMMA)
		
	def _sub_1_3_1_2(self):
		
		return _InclpathRule('name')
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(SEMICOLON)
		
class _MethodPropertiesRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'method_properties', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('properties')
		
		for child in astNode.getChildren():
			name = child.getName()
			if name == 'token':
				continue
			res.addChild(child)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(LSBRACKET)
		
	def _sub_1_2(self):
		
		return _MethodPropertyRule()
		
	def _sub_1_3(self):
		
		return grammar.zeroToMany(self._sub_1_3_1())
		
	def _sub_1_3_1(self):
		
		elements = []
		elements.append(self._sub_1_3_1_1())
		elements.append(self._sub_1_3_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_1_1(self):
		
		return grammar.tokenNode(COMMA)
		
	def _sub_1_3_1_2(self):
		
		return _MethodPropertyRule()
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(RSBRACKET)
		
class _GenumRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'genum', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('genum')
		
		name = astNode['#name']
		res.addChild(AstNode('name', name.getText()))
		
		for code in astNode.getChildrenByName('enum_code'):
			code.setId('')
			res.addChild(code)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		elements.append(self._sub_1_6())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_7)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_4(self):
		
		return _EnumCodeRule()
		
	def _sub_1_5(self):
		
		return grammar.zeroToMany(self._sub_1_5_1())
		
	def _sub_1_5_1(self):
		
		elements = []
		elements.append(self._sub_1_5_1_1())
		elements.append(self._sub_1_5_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_5_1_1(self):
		
		return grammar.tokenNode(COMMA)
		
	def _sub_1_5_1_2(self):
		
		return _EnumCodeRule()
		
	def _sub_1_6(self):
		
		return grammar.tokenNode(RBRACE)
		
class _AttrPropertiesRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'attr_properties', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('properties')
		
		for child in astNode.getChildren():
			name = child.getName()
			if name == 'token':
				continue
			res.addChild(child)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(LSBRACKET)
		
	def _sub_1_2(self):
		
		return _AttrPropertyRule()
		
	def _sub_1_3(self):
		
		return grammar.zeroToMany(self._sub_1_3_1())
		
	def _sub_1_3_1(self):
		
		elements = []
		elements.append(self._sub_1_3_1_1())
		elements.append(self._sub_1_3_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_1_1(self):
		
		return grammar.tokenNode(COMMA)
		
	def _sub_1_3_1_2(self):
		
		return _AttrPropertyRule()
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(RSBRACKET)
		
class _ImplementsStmtRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'implements_stmt', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('interfaces')
		
		for intf in astNode.getChildrenById('name'):
			intf.setId('')
			res.addChild(intf)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_11)
		
	def _sub_1_2(self):
		
		return _TypeNameRule('name')
		
	def _sub_1_3(self):
		
		return grammar.zeroToMany(self._sub_1_3_1())
		
	def _sub_1_3_1(self):
		
		elements = []
		elements.append(self._sub_1_3_1_1())
		elements.append(self._sub_1_3_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_1_1(self):
		
		return grammar.tokenNode(COMMA)
		
	def _sub_1_3_1_2(self):
		
		return _TypeNameRule('name')
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(SEMICOLON)
		
class _InheritsStmtRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'inherits_stmt', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('super_class')
		
		super_type = astNode['#super_name']
		super_type.setId('')
		res.addChild(super_type)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_9)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(KEY_10)
		
	def _sub_1_3(self):
		
		return _TypeNameRule('super_name')
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(SEMICOLON)
		
class _PropContentRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'prop_content', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		start.connect(self._sub_2()).connect(end)
		start.connect(self._sub_3()).connect(end)
		start.connect(self._sub_4()).connect(end)
		
	def transform(self, astNode):
		
		return astNode.getChildren()[0]
		
	def _sub_1(self):
		
		return self._sub_1_1()
		
	def _sub_1_1(self):
		
		return _PropTypeRule()
		
	def _sub_2(self):
		
		return self._sub_2_1()
		
	def _sub_2_1(self):
		
		return _PropAccessRule()
		
	def _sub_3(self):
		
		return self._sub_3_1()
		
	def _sub_3_1(self):
		
		return _PropDescriptionRule()
		
	def _sub_4(self):
		
		return self._sub_4_1()
		
	def _sub_4_1(self):
		
		return _PropGtypeRule()
		
class _ArgtypeRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'argtype', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		start.connect(self._sub_2()).connect(end)
		
	def transform(self, astNode):
		
		children = astNode.getChildren()
		
		if len(children) == 1:
			return children[0]
		else:
			res = AstNode(children[0].getText())
			res.addChild(children[2])
			return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		branches = []
		branches.append(self._sub_1_1_1())
		branches.append(self._sub_1_1_2())
		
		return grammar.Fork(branches)
		
	def _sub_1_1_1(self):
		
		return self._sub_1_1_1_1()
		
	def _sub_1_1_1_1(self):
		
		return grammar.tokenNode(KEY_41)
		
	def _sub_1_1_2(self):
		
		return self._sub_1_1_2_1()
		
	def _sub_1_1_2_1(self):
		
		return grammar.tokenNode(KEY_42)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(LPAR)
		
	def _sub_1_3(self):
		
		return _ArgtypeRule()
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(RPAR)
		
	def _sub_2(self):
		
		return self._sub_2_1()
		
	def _sub_2_1(self):
		
		return _TypeNameRule()
		
class _InParamRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'in_param', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		res.addChild(AstNode('name', astNode['#name'].getText()))
		type_ = astNode['#type']
		type_.setId('')
		res.addChild(type_)
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(LARROW)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(COLON)
		
	def _sub_1_4(self):
		
		return _ArgtypeRule('type')
		
class _InoutParamRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'inout_param', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		res.addChild(AstNode('name', astNode['#name'].getText()))
		type_ = astNode['#type']
		type_.setId('')
		res.addChild(type_)
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(LRARROW)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(COLON)
		
	def _sub_1_4(self):
		
		return _ArgtypeRule('type')
		
class _PropertyRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'property', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		name = astNode.getChildById('name')
		name.setId('')
		res.addChild(name)
		
		for content in astNode.getChildrenById('content'):
			content.setId('')
			res.addChild(content)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return _NameWDashesRule('name')
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_3(self):
		
		return grammar.zeroToOne(self._sub_1_3_1())
		
	def _sub_1_3_1(self):
		
		elements = []
		elements.append(self._sub_1_3_1_1())
		elements.append(self._sub_1_3_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_1_1(self):
		
		return _PropContentRule('content')
		
	def _sub_1_3_1_2(self):
		
		return grammar.zeroToMany(self._sub_1_3_1_2_1())
		
	def _sub_1_3_1_2_1(self):
		
		elements = []
		elements.append(self._sub_1_3_1_2_1_1())
		elements.append(self._sub_1_3_1_2_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_3_1_2_1_1(self):
		
		return grammar.tokenNode(COMMA)
		
	def _sub_1_3_1_2_1_2(self):
		
		return _PropContentRule('content')
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(RBRACE)
		
class _AttributeRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'attribute', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		res.addChild(AstNode('name', astNode['#name'].getText()))
		
		type_ = astNode['#type']
		type_.setId('')
		res.addChild(type_)
		
		props = astNode['properties']
		if props:
			res.addChild(props)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(COLON)
		
	def _sub_1_3(self):
		
		return _ArgtypeRule('type')
		
	def _sub_1_4(self):
		
		return grammar.zeroToOne(_AttrPropertiesRule())
		
	def _sub_1_5(self):
		
		return grammar.tokenNode(SEMICOLON)
		
class _OutParamRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'out_param', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		type_ = astNode['#type']
		type_.setId('')
		res.addChild(type_)
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(RARROW)
		
	def _sub_1_2(self):
		
		return _ArgtypeRule('type')
		
class _SignalRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'signal', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		name=astNode['#name']
		name.setId('')
		res.addChild(name)
		
		for p in astNode.getChildrenByName('in_param'):
			p.setId('')
			res.addChild(p)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return _NameWDashesRule('name')
		
	def _sub_1_2(self):
		
		return grammar.zeroToMany(_InParamRule())
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(SEMICOLON)
		
class _InterfaceMethodRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'interface_method', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		for child in astNode.getChildren():
			if child.getId() == 'name':
				res.addChild(AstNode('name', child.getText()))
			elif child.getName() != 'token':
				res.addChild(child)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_2(self):
		
		return grammar.zeroToOne(self._sub_1_2_1())
		
	def _sub_1_2_1(self):
		
		return self._sub_1_2_1_1()
		
	def _sub_1_2_1_1(self):
		
		branches = []
		branches.append(self._sub_1_2_1_1_1())
		branches.append(self._sub_1_2_1_1_2())
		
		return grammar.zeroToMany(grammar.Fork(branches))
		
	def _sub_1_2_1_1_1(self):
		
		return self._sub_1_2_1_1_1_1()
		
	def _sub_1_2_1_1_1_1(self):
		
		return _InParamRule()
		
	def _sub_1_2_1_1_2(self):
		
		return self._sub_1_2_1_1_2_1()
		
	def _sub_1_2_1_1_2_1(self):
		
		return _OutParamRule()
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(SEMICOLON)
		
class _MethodRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'method', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		for child in astNode.getChildren():
			if child.getId() == 'name':
				res.addChild(AstNode('name', child.getText()))
			elif child.getName() != 'token':
				res.addChild(child)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_2(self):
		
		return grammar.zeroToOne(_MethodPropertiesRule())
		
	def _sub_1_3(self):
		
		branches = []
		branches.append(self._sub_1_3_1())
		branches.append(self._sub_1_3_2())
		branches.append(self._sub_1_3_3())
		
		return grammar.zeroToMany(grammar.Fork(branches))
		
	def _sub_1_3_1(self):
		
		return self._sub_1_3_1_1()
		
	def _sub_1_3_1_1(self):
		
		return _InParamRule()
		
	def _sub_1_3_2(self):
		
		return self._sub_1_3_2_1()
		
	def _sub_1_3_2_1(self):
		
		return _InoutParamRule()
		
	def _sub_1_3_3(self):
		
		return self._sub_1_3_3_1()
		
	def _sub_1_3_3_1(self):
		
		return _OutParamRule()
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(SEMICOLON)
		
class _PropertiesRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'properties', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		for prop in astNode.getChildrenByName('property'):
			prop.setId('')
			res.addChild(prop)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_12)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_3(self):
		
		return grammar.zeroToMany(_PropertyRule())
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(RBRACE)
		
class _AttrSectionRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'attr_section', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		res.addChild(astNode['visibility'])
		
		for attr in astNode.getChildrenById('a'):
			attr.setId('')
			res.addChild(attr)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return _VisibilityRule()
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(KEY_33)
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_4(self):
		
		return grammar.zeroToMany(_AttributeRule('a'))
		
	def _sub_1_5(self):
		
		return grammar.tokenNode(RBRACE)
		
class _SignalsRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'signals', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		for s in astNode.getChildrenByName('signal'):
			s.setId('')
			res.addChild(s)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_31)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_3(self):
		
		return grammar.zeroToMany(_SignalRule())
		
	def _sub_1_4(self):
		
		return grammar.tokenNode(RBRACE)
		
class _GinterfaceRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'ginterface', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		name = AstNode('name', astNode['#name'].getText())
		res.addChild(name)
		
		for intfmethod in astNode.getChildrenByName('interface_method'):
			intfmethod.setId('')
			res.addChild(intfmethod)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_5)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_4(self):
		
		return grammar.zeroToMany(_InterfaceMethodRule())
		
	def _sub_1_5(self):
		
		return grammar.tokenNode(RBRACE)
		
class _MethodSectionRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'method_section', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		res.addChild(astNode['visibility'])
		
		for method in astNode.getChildrenById('m'):
			method.setId('')
			res.addChild(method)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return _VisibilityRule()
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(KEY_32)
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_4(self):
		
		return grammar.zeroToMany(_MethodRule('m'))
		
	def _sub_1_5(self):
		
		return grammar.tokenNode(RBRACE)
		
class _GobjectRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'gobject', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode(self.getName())
		
		res.addChild(AstNode('name', astNode['#name'].getText()))
		
		excluded_elements = ['token']
		for child in astNode.getChildren():
			if child.getName() in excluded_elements:
				continue
			child.setId('')
			res.addChild(child)
		
		return res
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_4)
		
	def _sub_1_2(self):
		
		return grammar.tokenNode(ID, 'name')
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_4(self):
		
		branches = []
		branches.append(self._sub_1_4_1())
		branches.append(self._sub_1_4_2())
		branches.append(self._sub_1_4_3())
		branches.append(self._sub_1_4_4())
		branches.append(self._sub_1_4_5())
		branches.append(self._sub_1_4_6())
		
		return grammar.zeroToMany(grammar.Fork(branches))
		
	def _sub_1_4_1(self):
		
		return self._sub_1_4_1_1()
		
	def _sub_1_4_1_1(self):
		
		return _InheritsStmtRule()
		
	def _sub_1_4_2(self):
		
		return self._sub_1_4_2_1()
		
	def _sub_1_4_2_1(self):
		
		return _ImplementsStmtRule()
		
	def _sub_1_4_3(self):
		
		return self._sub_1_4_3_1()
		
	def _sub_1_4_3_1(self):
		
		return _PropertiesRule()
		
	def _sub_1_4_4(self):
		
		return self._sub_1_4_4_1()
		
	def _sub_1_4_4_1(self):
		
		return _SignalsRule()
		
	def _sub_1_4_5(self):
		
		return self._sub_1_4_5_1()
		
	def _sub_1_4_5_1(self):
		
		return _MethodSectionRule()
		
	def _sub_1_4_6(self):
		
		return self._sub_1_4_6_1()
		
	def _sub_1_4_6_1(self):
		
		return _AttrSectionRule()
		
	def _sub_1_5(self):
		
		return grammar.tokenNode(RBRACE)
		
class _ModuleRule(grammar.Rule):

	def __init__(self, ident=''):
	
		grammar.Rule.__init__(self, 'module', ident)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		
		outer_module = None
		current_module = None
		
		module_name = astNode['module_name']
		for mname in module_name.getChildren():
			current_module = AstNode('module')
			current_module.addChild(AstNode('name', mname.getText()))
			if not outer_module:
				outer_module = current_module
			else:
				outer_module.addChild(current_module)
		
		elements = [
			'module',
			'type_declaration',
			'gobject',
			'ginterface',
			'gerror',
			'genum',
			'gflags'
			]
		for child in astNode.getChildren():
			if child.getName() in elements:
				child.setId('')
				current_module.addChild(child)
		
		return outer_module
		
		
	def _sub_1(self):
		
		elements = []
		elements.append(self._sub_1_1())
		elements.append(self._sub_1_2())
		elements.append(self._sub_1_3())
		elements.append(self._sub_1_4())
		elements.append(self._sub_1_5())
		
		return grammar.Sequence(elements)
		
	def _sub_1_1(self):
		
		return grammar.tokenNode(KEY_3)
		
	def _sub_1_2(self):
		
		return _ModuleNameRule()
		
	def _sub_1_3(self):
		
		return grammar.tokenNode(LBRACE)
		
	def _sub_1_4(self):
		
		branches = []
		branches.append(self._sub_1_4_1())
		branches.append(self._sub_1_4_2())
		branches.append(self._sub_1_4_3())
		branches.append(self._sub_1_4_4())
		branches.append(self._sub_1_4_5())
		branches.append(self._sub_1_4_6())
		branches.append(self._sub_1_4_7())
		
		return grammar.zeroToMany(grammar.Fork(branches))
		
	def _sub_1_4_1(self):
		
		return self._sub_1_4_1_1()
		
	def _sub_1_4_1_1(self):
		
		return _ModuleRule()
		
	def _sub_1_4_2(self):
		
		return self._sub_1_4_2_1()
		
	def _sub_1_4_2_1(self):
		
		return _TypeDeclarationRule()
		
	def _sub_1_4_3(self):
		
		return self._sub_1_4_3_1()
		
	def _sub_1_4_3_1(self):
		
		return _GobjectRule()
		
	def _sub_1_4_4(self):
		
		return self._sub_1_4_4_1()
		
	def _sub_1_4_4_1(self):
		
		return _GinterfaceRule()
		
	def _sub_1_4_5(self):
		
		return self._sub_1_4_5_1()
		
	def _sub_1_4_5_1(self):
		
		return _GerrorRule()
		
	def _sub_1_4_6(self):
		
		return self._sub_1_4_6_1()
		
	def _sub_1_4_6_1(self):
		
		return _GenumRule()
		
	def _sub_1_4_7(self):
		
		return self._sub_1_4_7_1()
		
	def _sub_1_4_7_1(self):
		
		return _GflagsRule()
		
	def _sub_1_5(self):
		
		return grammar.tokenNode(RBRACE)
		
class _GobjcreatorGrammar(grammar.Grammar):

	def __init__(self):
	
		grammar.Grammar.__init__(self, all_token_types)
		
	def expand(self, start, end, context):
		
		start.connect(self._sub_1()).connect(end)
		
	def transform(self, astNode):
		
		res = AstNode('gobjcreator')
		
		for child in astNode.getChildren():
			child.setId('')
			res.addChild(child)
		
		return res
		
	def _sub_1(self):
		
		return self._sub_1_1()
		
	def _sub_1_1(self):
		
		branches = []
		branches.append(self._sub_1_1_1())
		branches.append(self._sub_1_1_2())
		branches.append(self._sub_1_1_3())
		branches.append(self._sub_1_1_4())
		branches.append(self._sub_1_1_5())
		branches.append(self._sub_1_1_6())
		branches.append(self._sub_1_1_7())
		branches.append(self._sub_1_1_8())
		
		return grammar.zeroToMany(grammar.Fork(branches))
		
	def _sub_1_1_1(self):
		
		return self._sub_1_1_1_1()
		
	def _sub_1_1_1_1(self):
		
		return _IncludeStmtRule()
		
	def _sub_1_1_2(self):
		
		return self._sub_1_1_2_1()
		
	def _sub_1_1_2_1(self):
		
		return _ModuleRule()
		
	def _sub_1_1_3(self):
		
		return self._sub_1_1_3_1()
		
	def _sub_1_1_3_1(self):
		
		return _TypeDeclarationRule()
		
	def _sub_1_1_4(self):
		
		return self._sub_1_1_4_1()
		
	def _sub_1_1_4_1(self):
		
		return _GobjectRule()
		
	def _sub_1_1_5(self):
		
		return self._sub_1_1_5_1()
		
	def _sub_1_1_5_1(self):
		
		return _GinterfaceRule()
		
	def _sub_1_1_6(self):
		
		return self._sub_1_1_6_1()
		
	def _sub_1_1_6_1(self):
		
		return _GerrorRule()
		
	def _sub_1_1_7(self):
		
		return self._sub_1_1_7_1()
		
	def _sub_1_1_7_1(self):
		
		return _GenumRule()
		
	def _sub_1_1_8(self):
		
		return self._sub_1_1_8_1()
		
	def _sub_1_1_8_1(self):
		
		return _GflagsRule()
		
